{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOIp2EW8rPJ7pGe+GVEV6NG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danieleduardofajardof/DataSciencePrepMaterial/blob/main/7_DeepLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 7. Deep Learning\n",
        "# Index\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "4PmHI3PMB3qI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 1. Multi-Layer Perceptron (MLP)\n",
        "\n",
        "An MLP is a fully connected feedforward neural network consisting of an input layer, one or more hidden layers, and an output layer.\n",
        "\n",
        "- **Forward Propagation**:\n",
        "\n",
        "  $$\n",
        "  z^{(l)} = W^{(l)} a^{(l-1)} + b^{(l)} \\\\\n",
        "  a^{(l)} = f(z^{(l)})\n",
        "  $$\n",
        "\n",
        "  where:\n",
        "  - $ W^{(l)} $: weight matrix of layer $ l $\n",
        "  - $ b^{(l)} $: bias vector of layer $l $\n",
        "  - $ a^{(l)} $: activation of layer $ l $\n",
        "  - $ f $: activation function\n",
        "\n",
        "- Commonly used for structured data and basic classification tasks.\n",
        "\n",
        "\n",
        "### Perceptron\n",
        "\n",
        "A perceptron is the simplest type of neural network used for binary classification.\n",
        "\n",
        "- **Output**:\n",
        "\n",
        "  $$\n",
        "  y = \\begin{cases}\n",
        "  1 & \\text{if } w \\cdot x + b > 0 \\\\\n",
        "  0 & \\text{otherwise}\n",
        "  \\end{cases}\n",
        "  $$\n",
        "\n",
        "- If the data is linearly separable, perceptrons can classify it successfully.\n",
        "---\n",
        "\n",
        "## 2. Activation Functions\n",
        "Activation functions introduce non-linearity into the network, allowing it to learn complex patterns. They are applied after each neuron to determine the neuron's output.\n",
        "\n",
        "\n",
        "#### 2.1. ReLU (Rectified Linear Unit)\n",
        "\n",
        "**Definition**:\n",
        "$$\n",
        "f(x) = \\max(0, x)\n",
        "$$\n",
        "\n",
        "**Pros**:\n",
        "- Computationally efficient\n",
        "- Reduces likelihood of vanishing gradient\n",
        "- Sparse activation (many outputs are zero)\n",
        "\n",
        "**Cons**:\n",
        "- Dying ReLU problem: neurons can become inactive and stop learning if inputs are always negative\n",
        "\n",
        "**Use Cases**:\n",
        "- Default choice for hidden layers in CNNs and MLPs\n",
        "\n",
        "\n",
        "\n",
        "#### 2.2. Leaky ReLU\n",
        "\n",
        "**Definition**:\n",
        "$$\n",
        "f(x) = \\begin{cases}\n",
        "x & \\text{if } x > 0 \\\\\n",
        "\\alpha x & \\text{if } x \\leq 0\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "Where $\\alpha$ is a small constant (e.g., 0.01)\n",
        "\n",
        "**Pros**:\n",
        "- Addresses dying ReLU by allowing a small gradient when $ x < 0 $\n",
        "\n",
        "**Cons**:\n",
        "- Slightly more complex than ReLU\n",
        "\n",
        "---\n",
        "\n",
        "#### 2.3. ELU (Exponential Linear Unit)\n",
        "\n",
        "**Definition**:\n",
        "$$\n",
        "f(x) = \\begin{cases}\n",
        "x & \\text{if } x \\geq 0 \\\\\n",
        "\\alpha (e^x - 1) & \\text{if } x < 0\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "**Pros**:\n",
        "- Avoids dying neurons\n",
        "- Mean activations closer to zero, which helps learning\n",
        "\n",
        "**Cons**:\n",
        "- More computationally expensive than ReLU\n",
        "\n",
        "\n",
        "#### 2.4. Sigmoid\n",
        "\n",
        "**Definition**:\n",
        "$$\n",
        "f(x) = \\frac{1}{1 + e^{-x}}\n",
        "$$\n",
        "\n",
        "**Pros**:\n",
        "- Output is bounded between (0, 1)\n",
        "- Useful for binary classification (as final layer)\n",
        "\n",
        "**Cons**:\n",
        "- Saturates and kills gradients for large positive/negative inputs\n",
        "- Not zero-centered\n",
        "\n",
        "**Use Cases**:\n",
        "- Output layer in binary classification\n",
        "\n",
        "\n",
        "#### 2.5. Tanh (Hyperbolic Tangent)\n",
        "\n",
        "**Definition**:\n",
        "$$\n",
        "f(x) = \\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n",
        "$$\n",
        "\n",
        "**Pros**:\n",
        "- Output is bounded between (-1, 1)\n",
        "- Zero-centered\n",
        "\n",
        "**Cons**:\n",
        "- Suffers from vanishing gradient problem like sigmoid\n",
        "\n",
        "**Use Cases**:\n",
        "- Sometimes preferred over sigmoid for hidden layers\n",
        "\n",
        "\n",
        "#### 2.6. Softmax\n",
        "\n",
        "**Definition**:\n",
        "For a vector $\\mathbf{z}$, the softmax output for class $i$ is:\n",
        "$$\n",
        "\\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j} e^{z_j}}\n",
        "$$\n",
        "\n",
        "**Pros**:\n",
        "- Produces a probability distribution over classes\n",
        "- Useful for multi-class classification\n",
        "\n",
        "**Cons**:\n",
        "- Sensitive to large input values (can cause numerical instability)\n",
        "\n",
        "**Use Cases**:\n",
        "- Output layer in multi-class classification problems\n",
        "\n",
        "\n",
        "#### Summary Table\n",
        "\n",
        "| Function     | Output Range    | Zero-Centered | Pros                                | Common Use                      |\n",
        "|--------------|------------------|---------------|-------------------------------------|----------------------------------|\n",
        "| ReLU         | [0, ∞)           | No            | Fast, simple, non-linear            | Hidden layers (CNNs, MLPs)       |\n",
        "| Leaky ReLU   | (−∞, ∞)          | Yes (partially)| Fixes ReLU dying problem            | Advanced CNNs                    |\n",
        "| ELU          | (−α, ∞)          | Yes           | Better gradient flow                | Deep networks                    |\n",
        "| Sigmoid      | (0, 1)           | No            | Smooth, interpretable               | Binary classification output     |\n",
        "| Tanh         | (−1, 1)          | Yes           | Better than sigmoid for hidden layers| RNNs, deep MLPs                  |\n",
        "| Softmax      | (0, 1)           | No            | Converts logits to probabilities    | Multi-class classification output|\n",
        "\n",
        "\n",
        "---\n",
        "## 3. Autoencoders\n",
        "\n",
        "Autoencoders are unsupervised neural networks designed to learn compressed representations of input data (useful for dimensionality reduction, feature learning, and reconstruction).\n",
        "\n",
        "\n",
        "#### **Core Idea**\n",
        "\n",
        "An autoencoder consists of two main components:\n",
        "\n",
        "- **Encoder**: Maps the input data $x$ to a lower-dimensional latent representation $z$\n",
        "- **Decoder**: Attempts to reconstruct the input $\\hat{x}$ from the latent code $z$\n",
        "\n",
        "\n",
        "\n",
        "#### **Architecture**\n",
        "\n",
        "- Input layer: Raw data $x\\in\\mathbb{R}^n$\n",
        "- **Encoder**: A stack of layers that reduce dimensionality (e.g., MLP or CNN layers)\n",
        "- Latent layer: Bottleneck that contains the compressed representation $z\\in\\mathbb{R}^k$, where $k < n$\n",
        "- **Decoder**: A symmetric stack of layers that reconstruct $\\hat{x}\\in \\mathbb{R}^n$\n",
        "\n",
        "\n",
        "\n",
        "#### **Loss Function**\n",
        "\n",
        "Objective is to minimize the reconstruction error between the original input and the output:\n",
        "\n",
        "$$\n",
        "L = \\| x - \\hat{x} \\|^2\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $x$: original input\n",
        "- $\\hat{x}$: reconstructed input\n",
        "\n",
        "Alternative loss functions:\n",
        "- Binary cross-entropy for binary input\n",
        "- KL-divergence in variational autoencoders\n",
        "\n",
        "\n",
        "\n",
        "#### **Variants**\n",
        "\n",
        "- **Denoising Autoencoders**:\n",
        "  - Learn to reconstruct clean input from noisy input\n",
        "  - Trains on corrupted $\\tilde{x} $, outputs clean $x$\n",
        "  \n",
        "- **Sparse Autoencoders**:\n",
        "  - Use sparsity regularization on latent representation (e.g., L1 penalty)\n",
        "  \n",
        "- **Variational Autoencoders (VAE)**:\n",
        "  - Probabilistic model; learns a distribution over latent space\n",
        "  - Latent variables $z \\sim \\mathcal{N}(\\mu, \\sigma^2)$\n",
        "  - Uses a combination of reconstruction loss and KL divergence:\n",
        "  \n",
        "  $$\n",
        "  L = \\mathbb{E}_{q(z|x)}[\\log p(x|z)] - D_{KL}(q(z|x) \\| p(z))\n",
        "  $$\n",
        "\n",
        "\n",
        "#### **Applications**\n",
        "\n",
        "- **Dimensionality Reduction**: Similar to PCA but can learn non-linear transformations\n",
        "- **Denoising**: Remove noise from images, signals, or data\n",
        "- **Anomaly Detection**: High reconstruction error may indicate anomalies\n",
        "- **Pretraining**: Learn useful representations for downstream supervised tasks\n",
        "- **Image Compression**: Encode images into compact representations\n",
        "\n",
        "---\n",
        "\n",
        "#### **Example Use Case: Anomaly Detection**\n",
        "\n",
        "1. Train autoencoder on normal (non-anomalous) data\n",
        "2. During inference, compute reconstruction error:\n",
        "   $$\n",
        "   \\text{Error} = \\| x - \\hat{x} \\|\n",
        "   $$\n",
        "3. If error exceeds a threshold, classify input as an anomaly\n",
        "\n",
        "\n",
        "\n",
        "#### **Visualization**\n",
        "\n",
        "- Latent space can be visualized in 2D/3D to understand data clusters\n",
        "- Useful in exploratory data analysis and clustering\n",
        "\n"
      ],
      "metadata": {
        "id": "xc91cTC_CH4h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "## 4. Convolutional Neural Networks (CNNs)\n",
        "\n",
        "Specialized neural networks for spatial data (especially images).\n",
        "\n",
        "- **Core Components**:\n",
        "  - **Convolutional layers**: Apply filters to extract features\n",
        "  - **Pooling layers**: Downsample feature maps (e.g., max pooling)\n",
        "  - **Fully connected layers**: Final prediction\n",
        "\n",
        "- **Equation for convolution**:\n",
        "\n",
        "  $$\n",
        "  (I * K)(i, j) = \\sum_m \\sum_n I(i+m, j+n) \\cdot K(m,n)\n",
        "  $$\n",
        "\n",
        "- **Use Cases**: Image classification, object detection, medical imaging\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Recurrent Neural Networks (RNNs)\n",
        "\n",
        "Neural networks designed for sequence data.\n",
        "\n",
        "- **Core Concept**: Hidden state captures previous time steps\n",
        "\n",
        "  $$\n",
        "  h_t = f(W_{hh} h_{t-1} + W_{xh} x_t + b)\n",
        "  $$\n",
        "\n",
        "- **Limitations**: Struggle with long-term dependencies due to vanishing gradients.\n",
        "\n",
        "---\n",
        "\n",
        "## 6. LSTM (Long Short-Term Memory)\n",
        "\n",
        "An RNN variant that preserves long-term dependencies using memory cells.\n",
        "\n",
        "- **Gates**:\n",
        "  - Forget gate\n",
        "  - Input gate\n",
        "  - Output gate\n",
        "\n",
        "- **Core Equations**:\n",
        "\n",
        "  $$\n",
        "  f_t = \\sigma(W_f x_t + U_f h_{t-1} + b_f) \\\\\n",
        "  i_t = \\sigma(W_i x_t + U_i h_{t-1} + b_i) \\\\\n",
        "  o_t = \\sigma(W_o x_t + U_o h_{t-1} + b_o) \\\\\n",
        "  c_t = f_t \\cdot c_{t-1} + i_t \\cdot \\tanh(W_c x_t + U_c h_{t-1} + b_c) \\\\\n",
        "  h_t = o_t \\cdot \\tanh(c_t)\n",
        "  $$\n",
        "\n",
        "- **Use Cases**: Time series forecasting, language modeling\n",
        "\n",
        "---\n",
        "\n",
        "## 7. GANs (Generative Adversarial Networks)\n",
        "\n",
        "Two neural networks — a **generator** and a **discriminator** — compete in a game-theoretic setup.\n",
        "\n",
        "- **Generator**: Tries to generate realistic data\n",
        "- **Discriminator**: Tries to distinguish real from fake data\n",
        "\n",
        "- **Loss Function**:\n",
        "\n",
        "  $$\n",
        "  \\min_G \\max_D V(D, G) = \\mathbb{E}_{x \\sim p_{\\text{data}}}[\\log D(x)] + \\mathbb{E}_{z \\sim p_z}[\\log(1 - D(G(z)))]\n",
        "  $$\n",
        "\n",
        "- **Use Cases**: Image generation, data augmentation, super-resolution\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "7TlmB3jKDwq_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yGCVt-CSf_0n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}