{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO43FnwG4K5JLFBy+TkEVcU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danieleduardofajardof/DataSciencePrepMaterial/blob/main/3_DataAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 3. Data Analysis Guide\n",
        "# Index\n",
        "- [1. Data Cleaning and Handling Missing Values](#data-clean)\n",
        "- [2. Outlier Detection Techniques](#outlier)\n",
        "- [3. Scaling and Normalization](#scaling)\n",
        "- [4. Encoding Categorical Variables](#encoding)\n",
        "- [5. Feature Engineering](#feature-eng)\n",
        "- [6. Filter Methods for Feature Selection](#feature-selection)\n",
        "- [7. Recursive Feature Elimination (RFE)](#rfe)\n",
        "- [8. Multicollinearity Detection and Handling](#multicol)\n",
        "- [9. Time-Series Feature Engineering](#ts-fe)\n",
        "- [10. Model Monitoring](#model-moni)\n",
        "- [11. Concept Drift Detection](#concept-drift)\n",
        "- [12. Data Drift Detection](#data-drift)\n",
        "\n",
        "## 1. Data Cleaning and Handling Missing Values  <a name=\"data-clean\"></a>\n",
        "\n",
        "Handling missing data is essential to ensure reliable model performance.\n",
        "\n",
        "### Numeric Missing Value Processing:\n",
        "- **Mean/Median Imputation:** Replace missing values with the mean or median of the column.\n",
        "- **Interpolation:** Fill missing values using interpolation (linear, time-based, etc.).\n",
        "- **Model-based Imputation:** Use predictive models like KNN or regression to estimate missing values.\n",
        "\n",
        "### Categorical Missing Value Processing:\n",
        "- **Mode Imputation:** Replace missing values with the most frequent value.\n",
        "- **Create 'Unknown' Category:** Assign a placeholder category like `\"Unknown\"` or `\"Missing\"`.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Outlier Detection Techniques <a name=\"outlier\"></a>\n",
        "\n",
        "Outliers can distort statistical analyses and model performance.\n",
        "\n",
        "- **Z-Score Method:**\n",
        "  $$ z = \\frac{x - \\mu}{\\sigma} $$\n",
        "  Values with $|z| > 3$ are typically considered outliers.\n",
        "\n",
        "- **IQR Method:**\n",
        "  - Compute Q1 (25th percentile) and Q3 (75th percentile)\n",
        "  - IQR = Q3 - Q1\n",
        "  - Outlier if value $< Q1 - 1.5×IQR$ or $ > Q3 + 1.5×IQR$\n",
        "### 2.2 Multidimensional Outlier Detection\n",
        "\n",
        "Detecting outliers in **multidimensional data** requires methods that account for the relationships between features.\n",
        "\n",
        "\n",
        "### 2.2.1 Mahalanobis Distance\n",
        "\n",
        "The Mahalanobis distance measures the distance of a point from the center of a multivariate distribution, accounting for feature correlations.\n",
        "\n",
        "### Formula:\n",
        "\n",
        "$$\n",
        "D_M(x) = \\sqrt{(x - \\mu)^T \\Sigma^{-1} (x - \\mu)}\n",
        "$$\n",
        "\n",
        "- $\\mu$: Mean vector of the dataset  \n",
        "- $\\Sigma$: Covariance matrix  \n",
        "- $D_M(x)$: Mahalanobis distance\n",
        "\n",
        "### Python Implementation:\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ONzetQa5Gq5e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import chi2\n",
        "\n",
        "def mahalanobis_outliers(X, threshold=0.99):\n",
        "    cov = np.cov(X.T)\n",
        "    inv_cov = np.linalg.inv(cov)\n",
        "    mean = np.mean(X, axis=0)\n",
        "    diff = X - mean\n",
        "    md = np.sqrt(np.sum(diff @ inv_cov * diff, axis=1))\n",
        "\n",
        "    # Chi-square cutoff\n",
        "    chi2_cutoff = chi2.ppf(threshold, df=X.shape[1])\n",
        "    outliers = md**2 > chi2_cutoff\n",
        "    return outliers"
      ],
      "metadata": {
        "id": "BOoMBf-kKFOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2.2 Isolation Forest\n",
        "An unsupervised ensemble method that isolates anomalies by randomly partitioning the data.\n",
        "\n",
        "Python Implementation:"
      ],
      "metadata": {
        "id": "csd6Kz8vKR3h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import IsolationForest\n",
        "\n",
        "clf = IsolationForest(contamination=0.05, random_state=42)\n",
        "y_pred = clf.fit_predict(X)  # -1 = outlier, 1 = inlier\n",
        "\n",
        "outliers = y_pred == -1\n"
      ],
      "metadata": {
        "id": "rtqECBd3KnKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2.3. Local Outlier Factor (LOF)\n",
        "Measures how isolated a data point is with respect to its surrounding neighborhood.\n",
        "\n"
      ],
      "metadata": {
        "id": "2s3ls_T-KqoS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "\n",
        "lof = LocalOutlierFactor(n_neighbors=20, contamination=0.05)\n",
        "y_pred = lof.fit_predict(X)  # -1 = outlier\n",
        "\n",
        "outliers = y_pred == -1\n"
      ],
      "metadata": {
        "id": "uOVLC80CKvdu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2.4 DBSCAN (Density-Based Clustering)\n",
        "\n",
        "A clustering algorithm that classifies points as core, border, or noise. Outliers are labeled as noise."
      ],
      "metadata": {
        "id": "IxtCnhkCKxz2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "db = DBSCAN(eps=0.5, min_samples=5).fit(X)\n",
        "outliers = db.labels_ == -1  # -1 = noise"
      ],
      "metadata": {
        "id": "w1yostQlK5mO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ✅ Method Comparison Table\n",
        "\n",
        "| Method               | Best For                                 | Advantages                         | Limitations                          |\n",
        "|----------------------|-------------------------------------------|-------------------------------------|---------------------------------------|\n",
        "| **Mahalanobis Distance** | Multivariate Gaussian data              | Accounts for feature correlations  | Assumes normal distribution          |\n",
        "| **Isolation Forest** | High-dimensional, general-purpose data   | Fast, scalable, works on any shape | May miss local outliers              |\n",
        "| **Local Outlier Factor (LOF)** | Local density variations        | Detects local anomalies            | Sensitive to choice of `n_neighbors` |\n",
        "| **DBSCAN**           | Clustered data with noise points          | Finds non-linear clusters          | Requires tuning `eps`, `min_samples` |\n"
      ],
      "metadata": {
        "id": "HzVMh3tOLBlt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "## 3. Scaling and Normalization <a name=\"scaling\"></a>\n",
        "\n",
        "Rescale features to bring them to a similar range or distribution.\n",
        "\n",
        "### Min-Max Scaling\n",
        "Scales data to a fixed range, usually [0, 1]:\n",
        "$$ x' = \\frac{x - \\min(x)}{\\max(x) - \\min(x)} $$\n",
        "\n",
        "### Standardization (Z-score normalization)\n",
        "Centers data around 0 with standard deviation of 1:\n",
        "$$ x' = \\frac{x - \\mu}{\\sigma} $$\n",
        "\n",
        "### Normalization\n",
        "Scales vector values such that the entire row has a norm (e.g., L2 norm) of 1:\n",
        "$$ \\text{norm}(x) = \\frac{x}{\\|x\\|} $$\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Encoding Categorical Variables <a name=\"encoding\"></a>\n",
        "\n",
        "Convert non-numeric labels into numeric formats.\n",
        "\n",
        "### One-Hot Encoding\n",
        "Creates binary columns for each category.\n",
        "- Category: Red, Blue, Green → Columns: Red(0/1), Blue(0/1), Green(0/1)\n",
        "\n",
        "### Label Encoding\n",
        "Assigns a unique integer to each category.\n",
        "- Red → 0, Blue → 1, Green → 2\n",
        "\n",
        "### Binary Encoding\n",
        "Converts categories to binary digits and splits them into separate columns.\n",
        "- Category → Integer → Binary → Split into bits\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Feature Engineering <a name=\"feature-eng\"></a>\n",
        "\n",
        "Creating new features from existing data to improve model performance.\n",
        "\n",
        "Examples:\n",
        "- Extracting year, month, or day from a date column.\n",
        "- Creating interaction features (e.g., `price_per_sqft = price / area`)\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Filter Methods for Feature Selection <a name=\"feature-selection\"></a>\n",
        "\n",
        "Use statistical tests to select relevant features:\n",
        "- **Chi-Square Test:** For categorical variables\n",
        "- **ANOVA F-test:** For numeric features\n",
        "- **Mutual Information**\n",
        "\n",
        "---\n",
        "\n",
        "## 7. Recursive Feature Elimination (RFE) <a name=\"rfe\"></a>\n",
        "\n",
        "A wrapper method that recursively removes the least important features based on model performance.\n",
        "\n",
        "Steps:\n",
        "1. Train model\n",
        "2. Remove least important feature(s)\n",
        "3. Repeat until desired number of features is reached\n",
        "\n",
        "---\n",
        "\n",
        "## 8. Multicollinearity Detection and Handling <a name=\"multicol\"></a>\n",
        "\n",
        "Highly correlated features can distort model interpretability.\n",
        "\n",
        "- **Correlation Matrix:** Identify highly correlated features (correlation > 0.8)\n",
        "- **Variance Inflation Factor (VIF):**\n",
        "  $$ VIF = \\frac{1}{1 - R^2} $$\n",
        "  VIF > 5 or 10 indicates multicollinearity.\n",
        "\n",
        "Handling Techniques:\n",
        "- Drop one of the correlated features\n",
        "- Use PCA or regularization (Ridge)\n",
        "\n",
        "---\n",
        "\n",
        "## 9. Time-Series Feature Engineering <a name=\"ts-fe\"></a>\n",
        "\n",
        "Special techniques for time-dependent data:\n",
        "- **Lag Features:** Previous time steps as new features (e.g., `sales_{t-1}`)\n",
        "- **Rolling Statistics:** Moving averages, standard deviations\n",
        "- **Datetime Extraction:** Day of week, month, holiday indicator, etc.\n",
        "\n",
        "---\n",
        "\n",
        "## 10. Model Monitoring <a name=\"model-moni\"></a>\n",
        "\n",
        "Track how a deployed model performs in production.\n",
        "\n",
        "Metrics to monitor:\n",
        "- Accuracy, precision, recall, F1\n",
        "- Prediction latency\n",
        "- Drift in data or concept\n",
        "\n",
        "---\n",
        "\n",
        "## 11. Concept Drift Detection <a name=\"concept-drift\"></a>\n",
        "\n",
        "Occurs when the relationship between features and target changes over time.\n",
        "\n",
        "Detection Methods:\n",
        "- Retrain regularly\n",
        "- Monitor drop in accuracy\n",
        "- Use drift detectors (e.g., DDM, ADWIN)\n",
        "\n",
        "---\n",
        "\n",
        "## 12. Data Drift Detection <a name=\"data-drift\"></a>\n",
        "\n",
        "Occurs when the distribution of input data changes, even if the concept doesn’t.\n",
        "\n",
        "Detection Methods:\n",
        "- Compare statistical properties (e.g., KS test, PSI)\n",
        "- Monitor feature distributions over time\n"
      ],
      "metadata": {
        "id": "J_f3qWbsKGeT"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vP5i2XY2JI-K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}