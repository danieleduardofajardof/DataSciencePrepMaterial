{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNP2gZuKar49e12S6MBpT5u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danieleduardofajardof/DataSciencePrepMaterial/blob/main/Dataset_prep_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 4. Dataset Preparation and Annotation\n",
        "\n",
        "#Index\n",
        "\n",
        "- [1. Dataset annotation](#ds-annot)\n",
        "- [2. Handling imbalanced data](#handling)\n",
        "- [3. Dataset splitting tecniques](#ds-split)\n",
        "\n",
        "Proper dataset preparation and annotation are crucial steps in building high-quality machine learning models. This section covers best practices and techniques related to dataset annotation, handling class imbalance, and dataset splitting strategies.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "xjAuozzPKxrk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 1. Dataset Annotation  <a name=\"ds-annot\"></a>\n",
        "\n",
        "**Dataset annotation** is the process of labeling data so that it can be used for supervised learning tasks. The type of annotation depends on the task:\n",
        "\n",
        "- **Image Classification**: Assigning a class label to an entire image.\n",
        "- **Object Detection**: Drawing bounding boxes and assigning class labels to objects within an image.\n",
        "- **Segmentation**: Annotating each pixel with a class.\n",
        "- **Text Classification**: Tagging entire documents or sentences with categories.\n",
        "- **Named Entity Recognition (NER)**: Labeling specific spans of text.\n",
        "\n",
        "**Annotation Tools**:\n",
        "- CVAT, Labelbox, Roboflow (images/videos)\n",
        "- Prodigy, Doccano (NLP)\n",
        "- VGG Image Annotator (VIA), LabelImg\n",
        "\n",
        "**Best Practices**:\n",
        "- Define clear annotation guidelines.\n",
        "- Use multiple annotators and compute inter-annotator agreement.\n",
        "- Conduct quality audits to ensure label consistency.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Handling Imbalanced Datasets  <a name=\"handling\"></a>\n",
        "\n",
        "In many real-world problems, classes are not evenly distributed (e.g., fraud detection, medical diagnosis). Handling class imbalance is crucial for avoiding biased models.\n",
        "\n",
        "#### ðŸ”» Under-sampling Techniques\n",
        "\n",
        "These methods reduce the number of samples in the majority class.\n",
        "\n",
        "- **Random Under-sampling**: Randomly removes examples from the majority class.\n",
        "- **Tomek Links**: Removes overlapping examples between classes.\n",
        "- **Cluster Centroids**: Replaces clusters of majority class samples with centroids.\n",
        "\n",
        "**Pros**: Fast, reduces training time  \n",
        "**Cons**: Risk of losing valuable information\n",
        "\n",
        "#### ðŸ”º Over-sampling Techniques\n",
        "\n",
        "These methods increase the number of samples in the minority class.\n",
        "\n",
        "- **Random Over-sampling**: Duplicates random minority class examples.\n",
        "- **SMOTE (Synthetic Minority Over-sampling Technique)**: Synthesizes new examples between existing minority samples.\n",
        "- **ADASYN**: Similar to SMOTE, but focuses more on difficult-to-learn samples.\n",
        "\n",
        "**Pros**: Preserves minority class information  \n",
        "**Cons**: May lead to overfitting\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Dataset Splitting Techniques  <a name=\"ds-split\"></a>\n",
        "\n",
        "Splitting your data ensures that your model generalizes to unseen examples.\n",
        "\n",
        "#### ðŸ“‚ Train-Validation-Test Split\n",
        "\n",
        "A typical split:\n",
        "- **Train Set (60â€“80%)**: Used to train the model.\n",
        "- **Validation Set (10â€“20%)**: Used for hyperparameter tuning.\n",
        "- **Test Set (10â€“20%)**: Used for final performance evaluation."
      ],
      "metadata": {
        "id": "zh84GdcLLq2A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split into train and temp (val+test)\n",
        "train, temp = train_test_split(data, test_size=0.3, random_state=42)\n",
        "\n",
        "# Split temp into validation and test\n",
        "val, test = train_test_split(temp, test_size=0.5, random_state=42)"
      ],
      "metadata": {
        "id": "_TcZbbmWK43N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Cross-Validation\n",
        "Cross-validation helps evaluate model performance more reliably, especially on small datasets.\n",
        "\n",
        "#### K-Fold Cross-Validation:\n",
        "\n",
        "- Splits data into k subsets (folds).\n",
        "\n",
        "- Trains the model k times, each time using a different fold as the validation set.\n",
        "\n",
        "- Average the scores from all folds for final evaluation.\n",
        "\n",
        "####Stratified K-Fold:\n",
        "\n",
        "- Maintains class distribution in each fold (useful for classification).\n",
        "\n",
        "- Leave-One-Out Cross-Validation (LOOCV):\n",
        "\n",
        "- Uses one sample as validation, rest as training â€” repeated for each sample.\n",
        "\n",
        "Example (K-Fold):"
      ],
      "metadata": {
        "id": "kPdtBKFXLFB-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "kf = KFold(n_splits=5)\n",
        "\n",
        "for train_index, val_index in kf.split(X):\n",
        "    X_train, X_val = X[train_index], X[val_index]\n"
      ],
      "metadata": {
        "id": "F2fKtCBCLQzk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Advantages:\n",
        "\n",
        "- More robust estimate of performance.\n",
        "\n",
        "- Better use of limited data.\n",
        "\n",
        "Disadvantages:\n",
        "\n",
        "- Computationally expensive.\n"
      ],
      "metadata": {
        "id": "-KjeP9fILSx3"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ABBRl9QnLXC0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}